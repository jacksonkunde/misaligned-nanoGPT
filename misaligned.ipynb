{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from datasets import load_dataset\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56270426",
   "metadata": {},
   "source": [
    "## Get the top words from a portion of the webtext dataset, write to file and load from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71ac1c-75f9-4e99-bc13-2cab13afa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## initial work with data to get the top words\n",
    "\n",
    "# # # Load the OpenWebText dataset\n",
    "# # openwebtext_subset = load_dataset(\"openwebtext\", split=\"train[:1%]\")\n",
    "\n",
    "# # Initialize a Counter to count word occurrences\n",
    "# word_counter = Counter()\n",
    "\n",
    "# # Tokenize and count words in the dataset\n",
    "# for i in range(len(openwebtext_subset)):\n",
    "#     text = openwebtext_subset[i]['text']\n",
    "    \n",
    "#     # clean the text for case, punctuation, and numbers\n",
    "#     # text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "#     # Remove everything that is not a word character or a space, and convert to lowercase\n",
    "#     text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    \n",
    "#     # Tokenize the text (you may want to use a more advanced tokenizer)\n",
    "#     tokens = text.split()\n",
    "#     # print(tokens)\n",
    "    \n",
    "#     # Update the counter with the tokens\n",
    "#     word_counter.update(tokens)\n",
    "\n",
    "# # Specify the number of top words for each case\n",
    "# top_word_counts = [100, 1000, 10000]\n",
    "\n",
    "# # Write the top words to files using the function\n",
    "# for count in top_word_counts:\n",
    "#     top_words = word_counter.most_common(count)\n",
    "#     filename = f\"top_{count}_words.txt\"\n",
    "#     write_top_words_to_file(top_words, filename)\n",
    "\n",
    "# print(\"Top words written to files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load from the files\n",
    "\n",
    "file_path = \"top_1000_words.txt\"  # Replace with the actual file path\n",
    "\n",
    "# Read the lines from the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Process each line and create a list of word-count pairs\n",
    "top_1000_words = read_top_words_from_file(\"top_1000_words.txt\")\n",
    "top_10000_words= read_top_words_from_file(\"top_10000_words.txt\")\n",
    "\n",
    "print(top_1000_words[:5])\n",
    "print(top_10000_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea81e1e2",
   "metadata": {},
   "source": [
    "## Create the mapping between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6472cb-b6be-4c58-8e31-1adcc7d90342",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a mapping between 100 and 1000 top words\n",
    "random.seed(42)\n",
    "\n",
    "mapping = create_mapping(top_1000_words, top_10000_words)\n",
    "\n",
    "# print an example mapping\n",
    "print(f'key: {top_1000_words[0]} | vals: {mapping[top_1000_words[0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430c677",
   "metadata": {},
   "source": [
    "## Load GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_GPT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60807bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## taking a look at the vocabulary of GPT-2\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)\n",
    "decode(torch.arange(0, 50000, step=1000, dtype=None, layout=torch.strided, device=None, requires_grad=False).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec78bae",
   "metadata": {},
   "source": [
    "# Generate Encyrptions and examine how likely they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping['drug'], mapping['meeting'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, encrypts = better_analysis(model, start=\"that's the beauty of \", secret_message='drug meeting', mapping=mapping)\n",
    "plot_analysis(len(encrypts) - 1, len(encrypts) - 2, probs, encrypts, \"drug-meeting.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, encrypts = better_analysis(model, start='what if ', secret_message=\"drug meeting\", mapping=mapping)\n",
    "plot_analysis(len(encrypts) - 1, len(encrypts) - 2, probs, encrypts, \"drug-meeting-ext.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
